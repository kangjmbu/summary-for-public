# 判别模型和生成模型

------

生成模型：学习得到联合概率分布P(x,y)，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。

判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。

生成模型：隐马尔可夫模型 (HMM)，朴素贝叶斯等

判别模型：感知机 (线性分类模型)，支持向量机 (SVM)，决策树等



# Seq2Seq + Attention



# 决策树

信息熵，条件熵，信息增益

熵在决策树中起到绝对作用，贯穿决策树的中心，决策树最关键的环节是节点最优属性选择，依赖的标准就是熵

决策树剪枝策略（预剪枝和后剪枝）500问（65）

信息增益（64页）



# 随机森林

随机森林（双重随机性）样本、特征选择双重随机，相当于（决策树 + bagging）



# 支持向量机

分类算法（小样本，线性，非线性）

SVM是通过支持向量运算的分类器

寻找最合适的超平面从而实现分类

硬边界，软边界，核技巧（引入核函数）

支持非线性分类，结合使用拉格朗日乘子法及KKT条件，以及核函数生成非线性分类器

核函数作用：原坐标系里线性不可分的数据用核函数投影到另一高维空间，使新数据在新的空间中线性可分

核函数的引入避免了维数灾难，大大减少了计算量（数据从输入空间到特征空间的映射）

引入对偶问题：

对偶问题的对偶是原问题

对偶问题是凸优化问题

当满足一定条件时，原始问题与对偶问题的解是完全等价的

min-max(1/2 * w^2) -> max-min(1/2 * w^2)



# 为什么需要激活函数

引入非线性因素和复杂性

线性方程的复杂度有限，从数据中学习复杂函数映射的能力弱

使网络能力更加强大，增强网络的学习能力，使网络可以学习到 复杂的函数映射关系，更加深入的理解复杂的数据

sigmoid激活函数的倒数？ f(x) * (1 - f(x))

Relu激活函数（优点，如何理解其稀疏激活性） （109页）



# L1 (Lasso)和 L2正则化(Ridge)

惩罚力度，惩罚模型的复杂度

Regularization

正则化的本质是在代价函数中添加p范数

在Cost Function上添加了正则化项，能降低模型的过拟合程度

![](https://github.com/kangjmbu/summary-for-public/raw/main/image/6be93ac4537bd2f2ec37702542d2cd5.png)

L1正则化对所有参数的惩罚力度都一样，可以让一部分权重变为零，因此产生稀疏模型，能够去除某些特征（权重为0则等效于去除）。





![](https://github.com/kangjmbu/summary-for-public/raw/main/image/7df360dc93c0f6a17a102d2fbcb089f.png)



L2正则化减少了权重的固定比例，使权重平滑。L2正则化不会使权重变为0（不会产生稀疏模型），所以选择了更多的特征。

L1使权重稀疏，L2使权重平滑

正则化项能够降低模型的非线性程度，从而降低模型的过拟合程度

常见的防止过拟合的方法是在模型的损失函数中，需要对模型的参数进行“惩罚”，这样的话这些参数就不会太大，而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象



# Dropout

 防止过拟合 提升模型泛化能力

很少见到卷积层后接Dropout （原因主要是卷积层参数少，不易过拟合）

当相对于网络的复杂程度（即网络的表达能力、拟合能力）而言输入的数据量过小时，会出现过拟合，显然这时各神经元学习到的特征相互之间存在许多重复和冗余。

Dropout的直接作用是使部分神经元随机失活，从而减少冗余，减轻网络的过拟合现象。

过拟合现象：拟合曲线比较尖，不平滑，泛化能力不好



# BN和LN

归一化 normalization

为什么需要归一化处理？

   神经网络学习过程的本质就是为了学习数据分布，如果我们没有做归一化处理，那么每一批次训练数据的分布不一样，从大的方向上看，神经网络则需要在这多个分布中找到平衡点，从小的方向上看，由于每层网络输入数据分布在不断变化，这也会导致每层网络也在找平衡点，显然，神经网络就很难收敛了。

BN：1.计算出均值， 2.计算出方差，3.归一化处理到均值为0，方差为1  

 Batch Normalization(CNN, CV)：

   1.BN的计算就是把每个通道的B、H、W单独拿出来归一化处理

   2.针对每个channel我们都有一组γ,β，所以可学习的参数为2*C

   3.当batch size越小，BN的表现效果越不好，因为计算过程中所得到的均值和方差不能代表全局分布

Layer Normalization(RNN, NLP)：

   1.LN的计算就是把每个CHW单独拿出来归一化处理，不受batch_size的影响



一个batch中的每一个句子内部的K个词向量是有上下文关联的，所以做层归一化更加合理，又因为不同句子间的词向量关联性不大，批归一化不适用于NLP任务。

![](https://github.com/kangjmbu/summary-for-public/raw/main/image/2f9c1060ec908b62b88f168539cbf68.png)



# 优化器

SGD(Stochastic Gradient Descent)，随机梯度下降

SGDM(SGD with momentum)，它加入了动量机制，SGDM没有考虑对学习率进行自适应更新

Adagrad 自适应梯度下降

RMSProp 均方根传播

Adagrad有个致命问题，就是没有考虑到迭代衰减，RMSProp对这一点进行了改进修正，引入了 a / (1-a)

Adam：SGDM和RMSProp的结合

Adam（Adaptive momentum）是一种自适应动量的随机优化方法（A method for stochastic optimization）

Adam 优化算法需要做偏差修正（除以 1-b1 / 1-b2)

![](https://github.com/kangjmbu/summary-for-public/raw/main/image/4ae203419d2ce1aa1de72ebdea924a8.png)

mt：动量，根号vt：自适应调节学习率



五大优化器其实可以分为两类

SGD、SGDM

Adagrad、RMSProp、Adam

使用比较广的是SGDM和Adam

![](https://github.com/kangjmbu/summary-for-public/raw/main/image/e8b3481c61aa8051242563e63cc657d.png)

